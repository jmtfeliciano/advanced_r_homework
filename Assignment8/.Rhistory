permuted_shortest <- function(sample_size, blocks, seed) {
set.seed(seed)
master_list <- c()
block_list <- c()
while(length(master_list) < sample_size) {
current_block <- sample(blocks, size=1, replace=TRUE) # picks random number from block array
block_list <- c(block_list, current_block)
current_pool <- c(rep('A',current_block/2), rep('B',current_block/2)) # creates half As, half Bs
current_scrambled <- sample(current_pool) # scrambles series of A,Bs using `sample()`
master_list <- c(master_list, current_scrambled) # adds within-block treatments to the master treatment list
}
print(block_list)
print(master_list)
}
permuted_shortest(10,2,1)
sample(2, size=1)
sample(2, size=1)
sample(2, size=1)
sample(2, size=1)
c(1)
permuted_shortest(21,2,1)
permuted_shortest(22,2,1)
permuted_shortest(22,c(2,6),1)
permuted_shortest(22,c(2,6),1)
permuted_shortest(22,c(2,6),2)
permuted_shortest(22,c(2,6),2) == permuted_shortest(22,c(2,6),2)
permuted_shortest(22,c(2,6),2) == permuted_shortest(22,c(2,6),4)
permuted_shortest(22,c(2,6),2) == permuted_shortest(22,c(2,6),4)
permuted_shortest(22,c(2,6),4) == permuted_shortest(22,c(2,6),4)
permuted_shortest(22,c(2,6),4) == permuted_shortest(22,c(2,6),5)
permuted_shortest(22,c(2,6),4)
permuted_shortest(22,c(2,6),5)
results1 <- NA
results2 <- NA
results3 <- NA
results4 <- NA
blocks1 <- NA
blocks2 <- NA
blocks3 <- NA
blocks4 <- NA
permuted_shortest <- function(sample_size, blocks, seed) {
set.seed(seed)
master_list <- c()
# while-loop will continue until master list has equal or greater than desired sample size
# this randomly selects a block, create a random sample with that block size, then add it to the master list
while(length(master_list) < sample_size) {
current_block <- sample(blocks, size=1)
current_pool <- c(rep('A',current_block/2), rep('B',current_block/2)) # creates half As, half Bs
current_scrambled <- sample(current_pool) # scrambles series of A,Bs using `sample()`
master_list <- c(master_list, current_scrambled)
}
print(master_list)
}
permuted_shortest(10,2,1)
permuted_each <- function(sample_size, blocks, seed) {
set.seed(seed)
block_randomization <- c()
while(sum(block_randomization) < sample_size) {
block_randomization <- c(block_randomization, sample(blocks, size=1))
}
blocks2 <<- block_randomization
final_pool <- c()
for(i in 1:length(block_randomization)){
current_pool <- c(rep('A',block_randomization[i]/2), rep('B',block_randomization[i]/2))
sampled_pool <- sample(current_pool)
final_pool <- c(final_pool, sampled_pool)
}
# saving outside the scope so i can compare them below
results2 <<- final_pool
print(paste("and the first 6 treatments are ", toString(head(final_pool))))
}
permuted_min <- function(sample_size, blocks, seed) {
set.seed(seed)
block_randomization2 <- sample(blocks, size=sample_size/min(blocks), replace = TRUE)
last_index <- 0
for(i in 1:length(block_randomization2)){
if(sum(block_randomization2[1:i]) >= sample_size){
last_index <- i
break
}
}
blocks3 <<- block_randomization2[1:last_index]
final_pool_min <- c()
for(i in 1:last_index){
current_pool <- c(rep('A',block_randomization2[i]/2), rep('B',block_randomization2[i]/2))
sampled_pool <- sample(current_pool)
final_pool_min <- c(final_pool_min, sampled_pool)
}
# saving outside the scope so i can compare them below
results3 <<- final_pool_min
print(paste("and the first 6 treatments are ", toString(head(final_pool_min))))
}
permutted_eff <- function(sample_size, blocks, seed) {
set.seed(seed)
block_randomization <- sample(blocks, size=sample_size/min(blocks), replace = TRUE)
final_pool <- c()
for(i in 1:length(block_randomization)){
final_pool <- c(final_pool, c(rep('A',block_randomization[i]/2), rep('B',block_randomization[i]/2)))
if(length(final_pool) >= sample_size){
break;
}
}
results4 <<- final_pool
print(paste("and the first 6 treatments are ", toString(head(final_pool))))
}
all_call <- function(sample, blocks, seed){
permuted_shortest(sample, blocks, seed)
permuted_each(sample, blocks, seed)
permuted_min(sample, blocks, seed)
permutted_eff(sample, blocks, seed)
}
all_call(14,c(2,4,6), 121212)
# Josemari Feliciano
# HW 8
# Declares a changeable wd and sets working directory
wd<-"/Users/jfeliciano/Documents/advancedr/homework/Assignment8"
setwd(wd)
# it checks if dplyr is installed via require, if not, install it before loading.
if(!require("dplyr")){
install.packages("dplyr")
}
library("dplyr")
# Loads the file required for this homework
demo_data <- read.csv("Demographics.csv", header = TRUE)
test_master <- read.csv("TestScores_original.csv", header = TRUE)
test_makeup <- read.csv("TestScores_makeup.csv", header = TRUE)
# removes all the NA test scores -- this will prevent collision when merging combined data into wide format later
# this works for this case because students have taken at least 1 exam, so 100 students will be kept when deleting NAs
test_master <- test_master[!is.na(test_master$TestScore),]
test_master <- rbind(test_master, test_makeup)
# NOTE TO Dr. ESSERMAN: if you require something more robust than this NA trick, commented out right below is another way
# another way is here that works more robustly -- for loop
# For-loop goes row by row, repl
# for(row1 in 1:nrow(test_master)) {
#   for(row2 in 1:nrow(test_makeup)) {
#     if(test_makeup[row2,]$StudentID == test_master[row1,]$StudentID && test_makeup[row2,]$TestNumber == test_master[row1,]$TestNumber) {
#       test_master[row1,]$TestScore = test_makeup[row2,]$TestScore
#       break # exits inner for-loop since we already found it in the makeup file
#     }
#   }
# }
# Creates 'finaldata' df with the newly created wide-formatted dataset
finaldata <- reshape(test_master, timevar="TestNumber", idvar = "StudentID", v.names = "TestScore", direction="wide")
# Merges demo data into wide data by using StudentID
finaldata <- merge(demo_data, finaldata, by="StudentID", all=TRUE)
# my code replaces NAs with 0 ... goes over columns 1:5 which corresponds to test 1 - test 5 range
for(col in 1:5){
# uses the eval-parse-text trick for function/method calling for my own practice here, I could have made 5 separate calls or used sapply
eval(parse(text= paste0("finaldata$TestScore.",col,"[is.na(finaldata$TestScore.",col,")] <- 0") ))
}
#finds the lowest from tests 2 - 4, then uses that info to calculate the eventual final grade
finaldata$lowest24 <- apply(finaldata[c("TestScore.2", "TestScore.3", "TestScore.4")], 1, FUN = min)
finaldata$FinalGrade <- round(0.20*(finaldata$TestScore.2+finaldata$TestScore.3+finaldata$TestScore.4-finaldata$lowest24)+0.30*finaldata$TestScore.1+0.30*finaldata$TestScore.5,2)
# calculates class average, classdiff, majordiff, yeardiff inside the dataset
finaldata$ClassAverage <- mean(finaldata$FinalGrade)
finaldata <- finaldata %>%
mutate(ClassAverage = mean(FinalGrade), ClassDiff = round(FinalGrade - ClassAverage,2)) %>%
group_by(Major) %>%
mutate(MajorAverage = mean(FinalGrade), MajorDiff = round(FinalGrade-MajorAverage,2)) %>%
group_by(Year) %>%
mutate(YearAverage = mean(FinalGrade), YearDiff = round(FinalGrade-YearAverage,2))
# only keeps the variables required for the homework
finaldata <- select(finaldata, c("StudentID", "Year", "Major", "FinalGrade", "ClassDiff", "YearDiff", "MajorDiff"))
# labels the categorical variables
finaldata$Year <- factor(finaldata$Year, levels = c(1:5), labels = c("Freshman", "Sophomore", "Junior", "Senior", "Senior Plus"))
finaldata$Major <- factor(finaldata$Major, levels = c(1:6), labels = c("Chemistry", "Biology", "Mathematics", "Physics", "Psychology", "Other"))
# *******************************************************
# *******************REQUESTED OUTPUT********************
# *******************************************************c
# prints first 10 data which is already ordered by ID
head(finaldata, n=10)
# creates histogram of final grade
hist(finaldata$FinalGrade, xlab="Final Grade for Class (in %)", labels= TRUE, main="Final Grade Distribution of All Students", xlim=c(0,100), ylim=c(0,40))
(659.7-502)/502
(775.5-659.7)/659.7
(659.7-502)/502 / 5
(775.5-659.7)/659.7 / 6
(762.-659.7)/659.7 / 5
set.seed(12546) #set the seed
nsims<-10000 #set the number of simulations
sampmean<-NULL #initialize the vector to capture the sample means
n<-2 #set the number of samples drawn from our distribution
#Random uniform
for(i in 1:nsims){
x<-runif(n=n)
sampmean[i]<-mean(x)
}
mean(sampmean) #What do we expect this to be?
hist(sampmean,main=paste("Histogram of sample means for n=",n), col="green")
x=replicate(nsims,rexp(n=n))
sampmean<-apply(x,2, mean)
mean(sampmean) #What do we expect this to be?
sd(sampmean) #What do we expect this to be?
#Let's also look at the distribution of the sample means that are created
hist(sampmean,main=paste("Histogram of sample means for n=",n), col="green")
mean(sampmean) #What do we expect this to be?
par(mfrow=c(2,2))
set.seed(777444) #set the seed
sampmean<-NULL
x<-NULL
n=2 #set the number of samples drawn from our distribution
x=replicate(nsims,rexp(n=n))
sampmean<-apply(x,2, mean)
mean(sampmean) #What do we expect this to be?
sd(sampmean) #What do we expect this to be?
hist(sampmean,main=paste("Histogram of sample means for n=",n), col="green")
set.seed(125126127)
sampmean<-NULL
x<-NULL
n=5
x=replicate(nsims,rexp(n=n))
sampmean<-apply(x,2, mean)
mean(sampmean) #What do we expect this to be?
sd(sampmean) #What do we expect this to be?
hist(sampmean,main=paste("Histogram of sample means for n=",n), col="blue")
x=replicate(nsims,rexp(n=n))
sampmean<-apply(x,2, mean)
mean(sampmean) #What do we expect this to be?
sd(sampmean) #What do we expect this to be?
#Let's also look at the distribution of the sample means that are created
hist(sampmean,main=paste("Histogram of sample means for n=",n), col="green")
#Random Exponential
x=replicate(nsims,rexp(n=n))
sampmean<-apply(x,2, mean)
mean(sampmean) #What do we expect this to be?
sd(sampmean) #What do we expect this to be?
#Let's also look at the distribution of the sample means that are created
hist(sampmean,main=paste("Histogram of sample means for n=",n), col="green")
par(mfrow=c(2,2))
set.seed(777444) #set the seed
sampmean<-NULL
x<-NULL
n=2 #set the number of samples drawn from our distribution
#Random Exponential
x=replicate(nsims,rexp(n=n))
sampmean<-apply(x,2, mean)
mean(sampmean) #What do we expect this to be?
sd(sampmean) #What do we expect this to be?
#Let's also look at the distribution of the sample means that are created
hist(sampmean,main=paste("Histogram of sample means for n=",n), col="green")
#Let's look at n=5
set.seed(125126127)
sampmean<-NULL
x<-NULL
n=5
x=replicate(nsims,rexp(n=n))
sampmean<-apply(x,2, mean)
mean(sampmean) #What do we expect this to be?
sd(sampmean) #What do we expect this to be?
hist(sampmean,main=paste("Histogram of sample means for n=",n), col="blue")
#Let's look at n=10
set.seed(89652347)
sampmean<-NULL
x<-NULL
n=10
x=replicate(nsims,rexp(n=n))
sampmean<-apply(x,2, mean)
mean(sampmean) #What do we expect this to be?
sd(sampmean) #What do we expect this to be?
hist(sampmean,main=paste("Histogram of sample means for n=",n), col="red")
1/sqrt(2)
1/sqrt(5)
1/sqrt(10)
par(mfrow=c(2,2))
set.seed(777444) #set the seed
sampmean<-NULL
x<-NULL
n=2 #set the number of samples drawn from our distribution
par(mfrow=c(2,2))
set.seed(777444) #set the seed
sampmean<-NULL
x<-NULL
n=2 #set the number of samples drawn from our distribution
x=replicate(nsims,rexp(n=n))
sampmean<-apply(x,2, mean)
mean(sampmean) #What do we expect this to be?
sd(sampmean) #What do we expect this to be?
set.seed(32698578)
#number of simulations
nsims=1000
n.vector=c(25,50,100,500) #Look at a range of sample sizes
p.vector<-(1:50)/100 #range of the true proportions
y=list(p.vector,c("Exact", "Asymptotic", "Continuity Corrected"), n.vector) #Create the names for the array
#Initialize the array that will capture the coverage probabilities
cover.prob<-array(rep(NA, length(p.vector)*3*length(n.vector)),dim=c(length(p.vector),3, length(n.vector)),dimnames=y)
cover.prob<-array(rep(NA, length(p.vector)*3*length(n.vector)),dim=c(length(p.vector),3, length(n.vector)),dimnames=y)
cover.prob
install.packages("binom")
library(binom)
set.seed(32698578)
#number of simulations
nsims=1000
n.vector=c(25,50,100,500) #Look at a range of sample sizes
p.vector<-(1:50)/100 #range of the true proportions
y=list(p.vector,c("Exact", "Asymptotic", "Continuity Corrected"), n.vector) #Create the names for the array
#Initialize the array that will capture the coverage probabilities
cover.prob<-array(rep(NA, length(p.vector)*3*length(n.vector)),dim=c(length(p.vector),3, length(n.vector)),dimnames=y)
#This simulation loops over the n-vector and the p-vector
for(j in 1:length(n.vector)) {
for(i in 1:length(p.vector)){
#Get vector of length nsims containing the number of successes from the binomial distribution
# with size n and probability of succcess p
successes<-rbinom(nsims,size=n.vector[j],p=p.vector[i])
#Get the upper and lower bounds of each of the three types of confidence intervals
lower.exact<-binom.confint(successes,n.vector[j],method="exact")$lower
upper.exact<-binom.confint(successes,n.vector[j],method="exact")$upper
lower.asym<-binom.confint(successes,n.vector[j],method="asymptotic")$lower
upper.asym<-binom.confint(successes,n.vector[j],method="asymptotic")$upper
lower.cc<-lower.asym-0.5/n.vector[j]
upper.cc<-upper.asym+0.5/n.vector[j]
cover.prob[i,1,j]<-mean(lower.exact<=p.vector[i] & upper.exact>=p.vector[i])
cover.prob[i,2,j]<-mean(lower.asym<=p.vector[i] & upper.asym>=p.vector[i])
cover.prob[i,3,j]<-mean(lower.cc<=p.vector[i] & upper.cc>=p.vector[i])
}
}
cover.prob
c(1:10)
?quantile
quantile(c(1:10),0.2)
vec1 <- c(1:10)
quantile(vect1,0.2)
quantile(vec1,0.2)
vec1[ceiling(length(vect1))*0.2]
vec1[ceiling(length(vect1)*0.2)]
vec1[ceiling(length(vec1)*0.2)]
vec1
vec1 <- c(1:11)
vec1[ceiling(length(vec1)*0.2)]
quantile(vec1,0.2)
odd_length <- c(1,2,3,4,5)
quantile(odd_length,0.25)
odd_length[ceiling(length(vector1))*0.25]
odd_length[ceiling(length(odd_length))*0.25]
odd_length <- c(1,2,3,4,5)
quantile(odd_length,0.25)
odd_length[ceiling(length(odd_length))*0.25]
odd_length[floor(length(odd_length))*0.25]
odd_length[floor(length(odd_length))*0.25]
even_length <- c(1,2,3,4,5,6)
even_length[floor(length(even_length))*0.25]
quantile(even_length,0.25)
vector_even <- c(1:10)
vector_even
quantile(even_length,0.2)
?quantile
stats::quantile(even_length,0.2)
base::quantile
?base::quantile
?quantile
??quantile
x <- c(1:10)
x[ceiling(length(x)*0.2)]
quantile(x,0.2)
x <- c(1:10)
x[ceiling(length(x)*0.2)]
x
quantile(x,0.2)
x <- c(1:10)
quantile(x,0.2)
x[ceiling(length(x)*0.2)]
?quantile
x <- c(1:10)
quantile(x,0.2)
y <- c(x,11)
quantile(y,0.2)
y[ceiling(length(y)*0.2)]
x <- c(1:10)
quantile(x,0.5)
x[ceiling(length(x)*0.5)]
1:10
x <- c(1:10)
quantile(x,0.5)
x[ceiling(length(x)*0.5)]
c(1,2,3,4)
sample_vector <- c(1,2,3,4)
quantile(sample_vector,0.5)
> sample_vector[ceiling(length(> sample_vector)*0.5)]
> sample_vector[ceiling(length(sample_vector)*0.5)]
sample_vector[ceiling(length(sample_vector)*0.5)]
permuted <- function(sample_size, blocks, seed) {
# sets seed
set.seed(seed)
# initializes an empty vector
master_list <- c()
# while-loop will continue until master list has equal or greater than desired sample size
# this randomly selects a block, create a random sample with that block size, then add it to the master list
while(length(master_list) < sample_size) {
current_block <- sample(blocks, size=1, replace=TRUE)  # randomly selects a number from the block
current_pool <- c(rep('A',current_block/2), rep('B',current_block/2)) # creates half As, half Bs
current_scrambled <- sample(current_pool) # scrambles series of A,B's using `sample()`
master_list <- c(master_list, current_scrambled) # adds current sample to the end of master list
}
return(master_list)
}
permuted(10,c(2,3),10)
?set.seed
permuted(sample_size=8, blocks=c(2,4), seed=999)
permuted(sample_size=8, blocks=c(2,4), seed=999)
sample(c(1,2),1)
sample(c(1,2),1)
sample(c(1,2),1)
sample(c(1,repeat(2,99),1)
c(1,repeat(2,99))
c(1,repeat(c(2),99))
?repeat
;
?repeat
?
''
??repeat
''
c(1,rep(c(2),99))
lengthc(1,rep(c(2),99)))
lengthc(1,rep(c(2),99))
length(c(1,rep(c(2),99)))
c(1,repeat(2,99))
c(1,rep(c(2),99))
c(1,rep(c(2),99))
c(1,rep(c(2),99)
;
c(1,rep(c(2),99))
sample(c(1,repeat(2,99),1)
sample(c(1,repeat(2,99),1))
sample(c(1,rep(2,99),1))
sample(c(1,rep(2,99),1))
sample(c(1,rep(2,99),1))
sample(c(1,rep(2,99),1))
sample(c(1,rep(2,99),1))
sample(c(1,rep(2,99),1))
sample(c(1,rep(2,99),1))
sample(c(1,rep(2,99),1))
sample(c(1,rep(2,99),1))
sample(c(1,rep(2,99),1))
sample(c(1,rep(2,99),1))
sample(c(1,rep(2,99)))
sample(c(1,rep(2,99)))
sample(c(1,rep(2,99)),1)
sample(c(1,rep(2,99)),1)
sample(c(1,rep(2,99)),1)
sample(c(1,rep(2,99)),1)
sample(c(1,rep(2,99)),1)
sample(c(1,rep(2,99)),1)
sample(c(1,rep(2,99)),1)
sample(c(1,rep(2,99)),1)
sample(c(1,rep(2,99)),1)
sample(c(1,rep(2,99)),1)
sample(c(1,rep(2,99)),1)
sample(c(1,rep(2,99)),1)
sample(c(1,rep(2,99)),1)
sample(c(1,rep(2,99)),1)
sample(c(1,rep(2,99)),1)
sample(c(1,rep(2,99)),1)
sample(c(1,rep(2,99)),1)
sample(c(1,rep(2,99)),1)
sample(c(1,rep(2,99)),1)
original_data <- data.frame(names=c("a","b"),test=c(0,1,1,0),scores=c(90,89,0,12))
original_data
additional <- data.frame(names=c("b"),test=c(0),scores=c(11.11))
merge(original_data, additional, by=c(original_data,additional))
merge(original_data, additional, by=c("names","test"))
merge(original_data, additional, by=c("names","test"), all=TRUE)
?round
######################################################################################################
#Function: coverage
#Author: Denise Esserman
#Creation Date: November 24, 2015 (version 3.2.2)
#Modified: November 06, 2017 (version 3.4.2)
#Purpose: This function calculates the coverage probabilities for a one sample proportion using
#         three methods (1) exact, (2) asymptotic, (3) asymptotic with continuity correction
# Required Parameters:
#      n.vector = a vector of the sample sizes that we want to explore
#      p.vector= a vector of probabilities that we want to explore
#      seed = the seed to start the random number generator
#Optional Parameters:
#      nsims=the number of simulations to conduct - default is 1000
#Libraries: Requires the installation of binom package
#Output: Returns an array with the coverage probabilites.  Each matrix within the array
#represents a given sample size in n.vector.  The rows in the matrix are the probabilities we
#want to explore and the columns represent the 3 methods.
#Example: Calculate the coverage probabilities for two sample sizes and 5 proportions
#coverage(n.vector=c(30,100),p.vector=c(0.1,0.2,0.3,0.4,0.5), seed=121212)
########################################################################################################
coverage<-function(n.vector=n.vector, p.vector=p.vector, seed=seed, nsims=1000) {
#check to see if the package is already installed
if (!require("binom")) install.packages("binom")
require(binom) #load the package
set.seed(seed)
#nsims=nsims
y=list(p.vector,c("Exact", "Asymptotic", "Continuity Corrected"), n.vector)
cover.prob<-array(rep(NA, length(p.vector)*3*length(n.vector)),dim=c(length(p.vector),3, length(n.vector)),dimnames=y)
for(j in 1:length(n.vector)) {
for(i in 1:length(p.vector)){
#Get vector of length nsims containing the number of successes from the binomial distribution
# with size n and probability of succcess p
successes<-rbinom(nsims,size=n.vector[j],p=p.vector[i])
#Get the upper and lower bounds of each of the three types of confidence intervals
lower.exact<-binom.confint(successes,n.vector[j],method="exact")$lower
upper.exact<-binom.confint(successes,n.vector[j],method="exact")$upper
lower.asym<-binom.confint(successes,n.vector[j],method="asymptotic")$lower
upper.asym<-binom.confint(successes,n.vector[j],method="asymptotic")$upper
lower.cc<-lower.asym-0.5/n.vector[j]
upper.cc<-upper.asym+0.5/n.vector[j]
cover.prob[i,1,j]<-mean(lower.exact<=p.vector[i] & upper.exact>=p.vector[i])
cover.prob[i,2,j]<-mean(lower.asym<=p.vector[i] & upper.asym>=p.vector[i])
cover.prob[i,3,j]<-mean(lower.cc<=p.vector[i] & upper.cc>=p.vector[i])
}
}
return(cover.prob) #return the array
}
#Call coverage
output<-coverage(n.vector=c(30,100),p.vector=c(0.1,0.2,0.3,0.4,0.5), seed=121212)
output
